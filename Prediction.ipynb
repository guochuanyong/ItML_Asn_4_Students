{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from geopy import distance\n",
    "#!pip install geopandas\n",
    "import geopandas\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, InputLayer, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import metrics\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "def distance_km(x):\n",
    "    home = (x['lat'], x['long'])\n",
    "    merch = (x['merch_lat'], x['merch_long'])\n",
    "    return distance.distance(home, merch).km\n",
    "\n",
    "def NeuralNetworkPredict(url):\n",
    "    model = load_model('model')\n",
    "    df_pred = pd.read_csv(url)\n",
    "    df_pred.drop(columns={\"Unnamed: 0\",'is_fraud'}, inplace=True)\n",
    "\n",
    "    df_pred['distance_km'] = df_pred.apply(lambda x: distance_km(x), axis=1)\n",
    "    df_pred['trans_date_trans_time'] = pd.to_datetime(df_pred['trans_date_trans_time'])\n",
    "    df_pred['dob'] = pd.to_datetime(df_pred['dob'])\n",
    "\n",
    "    df_pred['age_transaction'] = df_pred.apply(lambda x: (x['trans_date_trans_time'] - x['dob']).days // 365, axis=1)\n",
    "\n",
    "    df_pred.drop(columns=['trans_date_trans_time','cc_num','merchant','first','last','street','lat','long','dob','trans_num','merch_lat','merch_long','job','city'],inplace=True)\n",
    "    df_pred = pd.get_dummies(df_pred,drop_first=True)\n",
    "\n",
    "    y_pred = model.predict(df_pred)\n",
    "    print(y_pred, y_pred.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40522/40522 [==============================] - 40s 984us/step\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#Input url or file name and run into function and run.\n",
    "NeuralNetworkPredict(\"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/fraudTrain.csv.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I Did:\n",
    "\n",
    "1. Calculated the distance between home and merchant using the latitude and longitude coordinates and created an new feature called 'distance_km'\n",
    "2. Converted 'trans_date_trans_time' and 'dob' into datetime datatype, then calculated the age of the person during the time of the transaction, new feature is created called 'age_transaction'\n",
    "3. Checked target balance, target was very imbalanced, which was expected for fraud transactions\n",
    "4. Dropped the features that were logically useless and likely had no correlation with fraud, also dropped features used to calculate distance_km and age_transaction\n",
    "5. To deal with imbalance, I calculated the log ratio of the frauds and non-frauds, and used this ratio as the bias_initializer parameter in the output layer of my neural network. This lets the neural network model deal with the imbalance in the data as it is training. \n",
    "6. Used standard scalar to normalize numerical data and one-hot encoding to encode categorical data\n",
    "7. Split data for testing and training, used neural network to train model\n",
    "8. Added dropout layer between dense layers of neural network to prevent overfitting. \n",
    "9. Saved model, loaded model in new .ipynb file\n",
    "10. Created function to used the trained model on new data to make predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
